# week-2-3---Regression-and-Classification
# 吴恩达机器学习：有监督学习 - 第二周和第三周笔记

## 第二周：多元线性回归与梯度下降

### 1. 多元线性回归概述

#### 1.1 单变量线性回归回顾
- 单变量线性回归：$f_w(x) = w_0 + w_1x$
- 目标：找到最优的参数 $w_0, w_1$，使得预测值与真实值之间的差异最小化
- 代价函数：$J(w_0, w_1) = \frac{1}{2m}\sum_{i=1}^{m}(f_w(x^{(i)}) - y^{(i)})^2$

#### 1.2 多元线性回归扩展
- 多元线性回归模型：$f_\mathbf{w}(\mathbf{x}) = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n$
- 向量化表示：$f_\mathbf{w}(\mathbf{x}) = \mathbf{w} \cdot \mathbf{x} = w_0 + \sum_{j=1}^{n}w_jx_j$
- 其中 $\mathbf{x} = [1, x_1, x_2, ..., x_n]^T$ 和 $\mathbf{w} = [w_0, w_1, w_2, ..., w_n]^T$
- $w_0$ 也称为偏置项(bias)或截距项(intercept)，$x_0 = 1$
- 目标：找到参数向量 $\mathbf{w}$，使得代价函数最小化

#### 1.3 多元线性回归代价函数
- 代价函数：$J(\mathbf{w}) = \frac{1}{2m}\sum_{i=1}^{m}(f_\mathbf{w}(\mathbf{x}^{(i)}) - y^{(i)})^2$
- 向量化形式：$J(\mathbf{w}) = \frac{1}{2m}(\mathbf{X}\mathbf{w} - \mathbf{y})^T(\mathbf{X}\mathbf{w} - \mathbf{y})$
- 其中 $\mathbf{X}$ 是设计矩阵，每行为一个训练样本的特征向量

### 2. 梯度下降算法

#### 2.1 梯度下降基本原理
- 梯度下降是一个优化算法，用于找到代价函数的最小值
- 基本思想：不断沿着代价函数的负梯度方向更新参数，直到收敛
- 迭代公式：$\mathbf{w} = \mathbf{w} - \alpha \nabla J(\mathbf{w})$
- 其中 $\alpha$ 是学习率，$\nabla J(\mathbf{w})$ 是代价函数关于参数的梯度

#### 2.2 多元线性回归的梯度下降
- 更新规则：
  - $w_j = w_j - \alpha \frac{\partial J(\mathbf{w})}{\partial w_j}$
  - $\frac{\partial J(\mathbf{w})}{\partial w_j} = \frac{1}{m}\sum_{i=1}^{m}(f_\mathbf{w}(\mathbf{x}^{(i)}) - y^{(i)})x_j^{(i)}$
- 向量化形式：$\mathbf{w} = \mathbf{w} - \alpha \frac{1}{m}\mathbf{X}^T(\mathbf{X}\mathbf{w} - \mathbf{y})$
- 同时更新所有参数 $w_j$

#### 2.3 梯度下降实践技巧

##### 2.3.1 特征缩放
- 目的：使所有特征的尺度相近，加速梯度下降收敛
- 常用方法：
  - 均值归一化：$x_j = \frac{x_j - \mu_j}{s_j}$
  - 最小-最大缩放：$x_j = \frac{x_j - \min(x_j)}{\max(x_j) - \min(x_j)}$
  - Z-score标准化：$x_j = \frac{x_j - \mu_j}{\sigma_j}$
- 目标：使得特征值通常在 -1 到 1 之间

##### 2.3.2 学习率选择
- 学习率太小：收敛很慢
- 学习率太大：可能不收敛，代价函数可能会振荡或发散
- 调试方法：绘制迭代次数与代价函数值的关系图
- 通常尝试的学习率范围：0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1

##### 2.3.3 批量梯度下降变种
- 批量梯度下降：使用所有训练样本计算梯度
- 随机梯度下降：每次使用一个样本计算梯度并更新参数
- 小批量梯度下降：每次使用一小批样本计算梯度并更新参数

### 3. 多项式回归

#### 3.1 基本概念
- 将线性回归扩展到非线性特征
- 通过添加特征的幂次项来捕捉非线性关系
- 例如：$f_\mathbf{w}(x) = w_0 + w_1x + w_2x^2 + w_3x^3 + ...$

#### 3.2 特征工程
- 手动创建新特征：例如 $x_2 = x_1^2$，$x_3 = x_1^3$ 等
- 特征交叉：创建特征的组合，如 $x_3 = x_1 \times x_2$
- 特征缩放在多项式回归中更为重要，因为高次幂特征的范围差异更大

### 4. 正规方程（Normal Equation）

#### 4.1 解析解
- 在某些情况下，可以直接求解使代价函数最小化的参数
- 正规方程：$\mathbf{w} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$
- 优点：不需要迭代，不需要选择学习率
- 缺点：当特征数量 n 很大时，计算 $(\mathbf{X}^T\mathbf{X})^{-1}$ 的复杂度为 $O(n^3)$，计算成本高

#### 4.2 梯度下降 vs 正规方程
- 梯度下降：
  - 需要选择学习率
  - 需要多次迭代
  - 当 n 很大时也能高效运行
- 正规方程：
  - 不需要选择学习率
  - 一次计算即可得到结果
  - 当 n 很大时计算效率低
  - 当 $\mathbf{X}^T\mathbf{X}$ 不可逆时需要特殊处理（如使用伪逆或正则化）

## 第三周：逻辑回归与正则化

### 1. 逻辑回归

#### 1.1 二元分类问题
- 目标：预测输出 y 为 0 或 1（两个类别）
- 例如：垃圾邮件检测、疾病诊断、欺诈检测等

#### 1.2 逻辑回归模型
- 模型：$f_\mathbf{w}(\mathbf{x}) = g(\mathbf{w} \cdot \mathbf{x})$
- 其中 $g(z) = \frac{1}{1+e^{-z}}$ 是 sigmoid 函数（也称为逻辑函数）
- 输出范围：(0, 1)，表示预测为类别 1 的概率
- 决策边界：当 $\mathbf{w} \cdot \mathbf{x} = 0$ 时，$f_\mathbf{w}(\mathbf{x}) = 0.5$

#### 1.3 逻辑回归代价函数
- 使用平方误差的问题：非凸函数，存在多个局部最小值
- 对数损失函数（Log Loss）：
  - $J(\mathbf{w}) = \frac{1}{m}\sum_{i=1}^{m}[-y^{(i)}\log(f_\mathbf{w}(\mathbf{x}^{(i)})) - (1-y^{(i)})\log(1-f_\mathbf{w}(\mathbf{x}^{(i)}))]$
- 这是一个凸函数，有唯一的全局最小值
- 基于最大似然估计推导得出

#### 1.4 逻辑回归的梯度下降
- 计算代价函数的梯度：
  - $\frac{\partial J(\mathbf{w})}{\partial w_j} = \frac{1}{m}\sum_{i=1}^{m}(f_\mathbf{w}(\mathbf{x}^{(i)}) - y^{(i)})x_j^{(i)}$
- 更新规则：$w_j = w_j - \alpha \frac{\partial J(\mathbf{w})}{\partial w_j}$
- 形式上与线性回归的梯度下降相同，但 $f_\mathbf{w}(\mathbf{x})$ 是不同的函数

### 2. 多类分类问题

#### 2.1 一对多（One-vs-All）策略
- 将 n 分类问题转化为 n 个二分类问题
- 每个分类器负责区分某一类与其他所有类
- 预测时，选择输出概率最高的类别作为最终预测

#### 2.2 实现步骤
1. 训练 k 个二元分类器，每个分类器 $f_i$ 预测 "类别 i" 和 "非类别 i"
2. 对于新样本 x，计算所有分类器的输出概率
3. 选择概率最高的类别：$\text{prediction} = \arg\max_i f_i(\mathbf{x})$

### 3. 正则化

#### 3.1 过拟合问题
- 模型在训练数据上表现很好，但在新数据上表现不佳
- 通常是因为模型过于复杂，捕捉了训练数据中的噪声
- 表现：高方差，较低偏差

#### 3.2 解决过拟合的方法
1. 收集更多数据
2. 特征选择：减少特征数量
3. 正则化：保留所有特征，但减小参数的值

#### 3.3 L2 正则化（Ridge 回归）
- 向代价函数添加参数的平方和的惩罚项
- 线性回归的正则化代价函数：
  - $J(\mathbf{w}) = \frac{1}{2m}\sum_{i=1}^{m}(f_\mathbf{w}(\mathbf{x}^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2m}\sum_{j=1}^{n}w_j^2$
- 逻辑回归的正则化代价函数：
  - $J(\mathbf{w}) = \frac{1}{m}\sum_{i=1}^{m}[-y^{(i)}\log(f_\mathbf{w}(\mathbf{x}^{(i)})) - (1-y^{(i)})\log(1-f_\mathbf{w}(\mathbf{x}^{(i)}))] + \frac{\lambda}{2m}\sum_{j=1}^{n}w_j^2$
- 注意：通常不对截距项 $w_0$ 进行正则化

#### 3.4 正则化与梯度下降
- 更新规则：
  - $w_0 = w_0 - \alpha \frac{1}{m}\sum_{i=1}^{m}(f_\mathbf{w}(\mathbf{x}^{(i)}) - y^{(i)})x_0^{(i)}$
  - $w_j = w_j - \alpha [\frac{1}{m}\sum_{i=1}^{m}(f_\mathbf{w}(\mathbf{x}^{(i)}) - y^{(i)})x_j^{(i)} + \frac{\lambda}{m}w_j]$ for $j \geq 1$
- 可以重写为：$w_j = w_j(1 - \alpha\frac{\lambda}{m}) - \alpha \frac{1}{m}\sum_{i=1}^{m}(f_\mathbf{w}(\mathbf{x}^{(i)}) - y^{(i)})x_j^{(i)}$
- 其中 $(1 - \alpha\frac{\lambda}{m})$ 项使得每次迭代参数都会略微减小（权重衰减）

#### 3.5 正则化与正规方程
- 正则化后的正规方程：$\mathbf{w} = (\mathbf{X}^T\mathbf{X} + \lambda \cdot \mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}$
- 其中 $\mathbf{I}$ 是单位矩阵，但左上角的元素（对应 $w_0$）设为 0
- 正则化可以解决 $\mathbf{X}^T\mathbf{X}$ 不可逆的问题

### 4. 模型评估与选择

#### 4.1 训练集、验证集与测试集
- 训练集：用于训练模型的数据
- 验证集：用于调整超参数（如正则化系数 $\lambda$）的数据
- 测试集：用于评估模型泛化性能的数据

#### 4.2 交叉验证
- k-fold交叉验证：将训练数据分为 k 个子集，每次用 k-1 个子集训练，剩下的一个子集验证
- 可以更准确地评估模型性能，减少对特定训练/验证集划分的依赖

#### 4.3 学习曲线
- 训练误差与训练样本数量的关系
- 验证误差与训练样本数量的关系
- 有助于诊断偏差-方差问题：
  - 高偏差（欠拟合）：训练误差和验证误差都很高，且很接近
  - 高方差（过拟合）：训练误差低，验证误差高，两者差距大

#### 4.4 偏差-方差权衡
- 模型复杂度增加：偏差减小，方差增大
- 模型复杂度减小：偏差增大，方差减小
- 正则化参数 $\lambda$ 的选择就是一种偏差-方差权衡

### 5. 决策边界可视化

#### 5.1 线性决策边界
- 当特征为线性组合时，决策边界是一条直线（二维特征）或超平面（高维特征）
- 方程：$w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n = 0$

#### 5.2 非线性决策边界
- 通过添加高阶多项式特征，可以得到非线性决策边界
- 例如：圆形决策边界可以通过添加 $x_1^2 + x_2^2$ 形式的特征来实现
- 更复杂的边界可以通过更高阶多项式或特征组合实现

### 总结

- 多元线性回归扩展了单变量线性回归，可以处理多个特征
- 梯度下降是一种优化算法，用于找到使代价函数最小的参数
- 特征缩放和学习率选择对梯度下降的收敛速度有重要影响
- 逻辑回归是一种处理二元分类问题的算法，输出值介于0和1之间
- 正则化通过添加惩罚项来防止过拟合，平衡了模型的偏差和方差
- 交叉验证帮助我们选择最佳的正则化参数和其他超参数
- 通过添加多项式特征，可以用线性模型拟合非线性关系

