### 1. 多元线性回归概述

#### 1.1 单变量线性回顾
单变量线性回归模型：  
$$ f_w(x) = w_0 + w_1 x $$

目标：找到最优的参数 $w_0, w_1$，使得预测值与真实值之间的差异最小化。  
代价函数：  
$$ J(w_0, w_1) = \frac{1}{2m} \sum_{i=1}^{m} \left( f_w(x^{(i)}) - y^{(i)} \right)^2 $$

#### 1.2 多元线性回归模型
多元线性回归引入多个特征变量：$x_1, x_2, \dots, x_n$  
模型表达式：  
$$ f_{\vec{w}}(\vec{x}) = w_0 + w_1 x_1 + w_2 x_2 + \dots + w_n x_n $$

向量化表示：  
$$ f_{\vec{w}}(\vec{x}) = \vec{w} \cdot \vec{x} $$

其中  
$$ \vec{w} = \begin{bmatrix} w_0 \\ w_1 \\ w_2 \\ \vdots \\ w_n \end{bmatrix}, \quad \vec{x} = \begin{bmatrix} 1 \\ x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} $$  
（注意：$x_0 = 1$，称为截距项）

#### 1.3 多元线性回归代价函数
代价函数：  
$$ J(\vec{w}) = \frac{1}{2m} \sum_{i=1}^{m} \left( f_{\vec{w}}(\vec{x}^{(i)}) - y^{(i)} \right)^2 $$

目标：找到参数向量 $\vec{w}$，使代价函数最小化。  
向量化形式：  
$$ J(\vec{w}) = \frac{1}{2m} \left( \mathbf{X} \vec{w} - \vec{y} \right)^T \left( \mathbf{X} \vec{w} - \vec{y} \right) $$  
其中 $\mathbf{X}$ 是设计矩阵，每行为一个训练样本的特征向量。

---

### 2. 梯度下降算法

#### 2.1 梯度下降基本原理
梯度下降是一种优化算法，用于找到代价函数的最小值。  
基本思想：沿着代价函数的负梯度方向更新参数，直到收敛。  
迭代公式：  
$$ \vec{w} := \vec{w} - \alpha \nabla J(\vec{w}) $$  
其中 $\alpha$ 是学习率，控制每次更新的步长，$\nabla J(\vec{w})$ 是代价函数对参数的梯度。

#### 2.2 多元线性回归的梯度下降
对每个参数的更新规则：  
$$ w_j := w_j - \alpha \frac{\partial}{\partial w_j} J(\vec{w}) $$  
$$ \frac{\partial}{\partial w_j} J(\vec{w}) = \frac{1}{m} \sum_{i=1}^{m} \left( f_{\vec{w}} (\vec{x}^{(i)}) - y^{(i)} \right) x_j^{(i)} $$

向量化形式：  
$$ \vec{w} := \vec{w} - \alpha \frac{1}{m} \mathbf{X}^T (\mathbf{X} \vec{w} - \vec{y}) $$  
注意：所有参数 $w_j$ 必须同时更新。

#### 2.3 特征缩放
问题：当特征的取值范围差异很大时，梯度下降收敛变慢。  
解决方案：特征缩放使所有特征具有相近的取值范围。  
常用方法：  
均值归一化：  
$$ x_j := \frac{x_j - \mu_j}{s_j} $$  
其中 $\mu_j$ 是特征 $j$ 的平均值，$s_j$ 是特征 $j$ 的范围（最大值减最小值）或标准差。  
目标：使特征值大致在 $-1 \leq x_j \leq 1$ 范围内。

#### 2.4 学习率选择
- 学习率太小：收敛太慢。
- 学习率太大：可能不收敛，代价函数可能震荡或发散。

如何选择：  
绘制迭代次数与代价函数 $J(\vec{w})$ 的关系图，代价函数应该在每次迭代后减小。  
尝试不同的学习率：$0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, \dots$。

---

### 3. 线性回归的特征工程与多项式回归

#### 3.1 特征工程
定义：从现有数据创建新特征，以提高模型性能。  
示例：根据房屋宽度和深度创建面积特征。  
重要性：良好的特征选择和创建可以极大地提高模型效果。

#### 3.2 多项式回归
通过添加现有特征的幂次项来捕捉非线性关系。  
示例：  
$$ f_{\vec{w}}(x) = w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \dots $$

实现：将 $x^2, x^3, \dots$ 作为新特征添加到模型中。  
特征缩放在多项式回归中更为重要，因为不同幂次的特征范围差异更大。

---

### 4. 正规方程法

#### 4.1 解析解
批量梯度下降是一种迭代方法，而正规方程法直接求解。  
正规方程：  
$$ \vec{w} = \left( \mathbf{X}^T \mathbf{X} \right)^{-1} \mathbf{X}^T \vec{y} $$  
优点：不需要迭代、不需要选择学习率、不需要特征缩放。  
缺点：当特征数量很大时，计算 $\left( \mathbf{X}^T \mathbf{X} \right)^{-1}$ 的复杂度为 $O(n^3)$，效率低下。

#### 4.2 梯度下降 vs 正规方程
- **梯度下降**：  
  - 需要选择学习率。  
  - 需要多次迭代。  
  - 特征数量大时表现良好。  
  - 适用于大规模数据。

- **正规方程**：  
  - 不需要选择学习率。  
  - 不需要迭代。  
  - 特征数量大时计算复杂度高。  
  - 适用于特征数量较少的情况（特征数量阈值约为10,000）。

#### 4.3 不可逆矩阵情况
当 $\mathbf{X}^T \mathbf{X}$ 不可逆（奇异矩阵）时可能出现问题。  
不可逆的常见原因：  
- 特征之间线性相关（共线性）。  
- 特征数量大于样本数量。

解决方案：  
- 删除冗余特征。  
- 使用正则化。  
- 使用伪逆（如 NumPy 中的 `pinv` 函数）。

---

### 5. 特征选择和模型评估

#### 5.1 特征选择
目标：选择最相关、最有预测力的特征。  
方法：
- 领域知识。
- 特征重要性分析。
- 正则化技术（将在第三周讨论）。
- 逐步特征选择（前向选择、后向消除）。

#### 5.2 模型评估
- **训练误差**：模型在训练数据上的表现。
- **泛化误差**：模型在未见过的数据上的表现。

验证集和测试集：评估模型性能的独立数据集。  
常用评估指标：
- 均方误差（MSE）
- 平均绝对误差（MAE）
- R² 决定系数
