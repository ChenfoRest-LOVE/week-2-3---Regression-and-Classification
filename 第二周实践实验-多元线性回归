import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm

def load_data(filename):
    data = np.loadtxt(filename, delimiter=',')
    X = data[:, :-1]
    y = data[:, -1]
    return X, y

print("加载数据...")
try:
    X, y = load_data('data.txt')
except:
    print("未找到数据文件，创建示例数据...")
    np.random.seed(42)
    X = np.random.rand(100, 2)
    X[:, 0] = X[:, 0] * 1000 + 500
    X[:, 1] = np.round(X[:, 1] * 3 + 1)
    y = 50 * X[:, 0] + 10000 * X[:, 1] + np.random.randn(100) * 10000
    
m = len(y)
n = X.shape[1]

print(f"数据集包含 {m} 个样本，每个样本有 {n} 个特征")
print(f"前5条样本数据:")
for i in range(min(5, m)):
    print(f"样本 {i+1}: 特征 = {X[i]}, 价格 = {y[i]:.2f}")

print("\n绘制数据分布...")
if n == 1:
    plt.figure(figsize=(10, 6))
    plt.scatter(X, y, marker='x', c='red', s=30)
    plt.xlabel('特征')
    plt.ylabel('价格')
    plt.title('房价数据分布')
    plt.show()
elif n == 2:
    fig = plt.figure(figsize=(12, 8))
    ax = fig.add_subplot(111, projection='3d')
    ax.scatter(X[:, 0], X[:, 1], y, c='r', marker='o')
    ax.set_xlabel('面积')
    ax.set_ylabel('卧室数')
    ax.set_zlabel('价格')
    ax.set_title('房价与面积、卧室数的关系')
    plt.show()

def feature_normalize(X):
    X_norm = X.copy()
    mu = np.zeros(X.shape[1])
    sigma = np.zeros(X.shape[1])
    
    for i in range(X.shape[1]):
        mu[i] = np.mean(X[:, i])
        sigma[i] = np.std(X[:, i])
        X_norm[:, i] = (X[:, i] - mu[i]) / sigma[i]
    
    return X_norm, mu, sigma

print("\n执行特征缩放和归一化...")
X_norm, mu, sigma = feature_normalize(X)
print("特征均值:", mu)
print("特征标准差:", sigma)
print("归一化后的前5条样本数据:")
for i in range(min(5, m)):
    print(f"样本 {i+1}: 归一化特征 = {X_norm[i]}")

X_b = np.c_[np.ones((m, 1)), X_norm]
n_b = X_b.shape[1]

def compute_cost(X, y, theta):
    m = len(y)
    predictions = X.dot(theta)
    errors = predictions - y
    J = (1/(2*m)) * np.sum(errors**2)
    return J

def batch_gradient_descent(X, y, theta, alpha, num_iters):
    m = len(y)
    J_history = np.zeros(num_iters)
    
    for i in range(num_iters):
        predictions = X.dot(theta)
        errors = predictions - y
        gradients = (1/m) * X.T.dot(errors)
        theta = theta - alpha * gradients
        J_history[i] = compute_cost(X, y, theta)
        
    return theta, J_history

print("\n初始化梯度下降...")
initial_theta = np.zeros(n_b)
iterations = 1500
alpha = 0.01

print(f"运行梯度下降（学习率={alpha}，迭代次数={iterations}）...")
theta, J_history = batch_gradient_descent(X_b, y, initial_theta, alpha, iterations)

print("\n梯度下降后的参数值:")
for i, t in enumerate(theta):
    feature_name = "截距" if i == 0 else f"特征{i}"
    print(f"{feature_name}: {t:.6f}")

final_cost = compute_cost(X_b, y, theta)
print(f"最终代价: {final_cost:.6f}")

plt.figure(figsize=(10, 6))
plt.plot(range(iterations), J_history)
plt.xlabel('迭代次数')
plt.ylabel('代价函数 J(θ)')
plt.title('梯度下降的收敛过程')
plt.grid(True)
plt.show()

print("\n比较不同学习率的性能...")
alphas = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3]
J_histories = []

plt.figure(figsize=(12, 8))

for i, alpha in enumerate(alphas):
    theta_temp = np.zeros(n_b)
    theta_temp, J_history = batch_gradient_descent(X_b, y, theta_temp, alpha, iterations)
    J_histories.append(J_history)
    plt.plot(range(iterations), J_history, label=f'α = {alpha}')

plt.xlabel('迭代次数')
plt.ylabel('代价函数 J(θ)')
plt.title('不同学习率的梯度下降收敛情况')
plt.legend()
plt.grid(True)
plt.show()

def normal_equation(X, y):
    return np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)

print("\n使用正规方程求解参数...")
X_original_b = np.c_[np.ones((m, 1)), X]
theta_normal = normal_equation(X_original_b, y)

print("正规方程的参数结果:")
for i, t in enumerate(theta_normal):
    feature_name = "截距" if i == 0 else f"特征{i}"
    print(f"{feature_name}: {t:.6f}")

def predict(X, theta, mu=None, sigma=None):
    X_copy = X.copy()
    
    if mu is not None and sigma is not None:
        for i in range(X.shape[1]):
            X_copy[:, i] = (X[:, i] - mu[i]) / sigma[i]
    
    X_copy = np.c_[np.ones((X_copy.shape[0], 1)), X_copy]
    
    return X_copy.dot(theta)

print("\n在训练集上评估模型...")
y_pred_gd = predict(X, theta, mu, sigma)
y_pred_normal = X_original_b.dot(theta_normal)

def mse(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

mse_gd = mse(y, y_pred_gd)
mse_normal = mse(y, y_pred_normal)

print(f"梯度下降的均方误差: {mse_gd:.6f}")
print(f"正规方程的均方误差: {mse_normal:.6f}")

if n == 1:
    plt.figure(figsize=(10, 6))
    plt.scatter(X, y, marker='x', color='red', label='实际数据')
    X_plot = np.linspace(min(X), max(X), 100).reshape(-1, 1)
    y_plot = predict(X_plot, theta, mu, sigma)
    plt.plot(X_plot, y_plot, '-', color='blue', label='梯度下降预测')
    y_plot_normal = np.c_[np.ones(100), X_plot].dot(theta_normal)
    plt.plot(X_plot, y_plot_normal, '--', color='green', label='正规方程预测')
    plt.xlabel('特征')
    plt.ylabel('价格')
    plt.title('线性回归: 实际值 vs 预测值')
    plt.legend()
    plt.grid(True)
    plt.show()

print("\n使用模型进行新预测...")
if n == 2:
    x_new = np.array([1200, 3])
    price_pred_gd = predict(x_new.reshape(1, -1), theta, mu, sigma)[0]
    price_pred_normal = np.append(1, x_new).dot(theta_normal)
    
    print(f"新房子：面积 = {x_new[0]}平方英尺，卧室数 = {x_new[1]}")
    print(f"梯度下降预测价格: ${price_pred_gd:.2f}")
    print(f"正规方程预测价格: ${price_pred_normal:.2f}")

print("\n实验完成！")
